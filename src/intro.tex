
\chapter{Introduction}


While often overlooked, program size can be a first-order constraint.
From tiny embedded devices up to cloud servers, these systems are all operating under limited addressable memory, storage, or bandwidth. When the program becomes excessively large relative to the given constraints, this has a detrimental effect on the system.
%In the extreme, this means failure.
This is very likely to happen as programs gain new features over time, continuously growing in size and complexity~\cite{lavaee19,chabbi21}.
In such scenarios, reducing the application footprint is essential~\cite{schultz03,varma04,sehgal12,keoh14,auler17,chabbi21}.

% %Code size is a critical issue whenever the program size becomes too large for the available resources.
% Program size is a critical issue whenever it becomes excessively large relative to given constraints such as the addressable memory space, storage size, download bandwidth, etc.
% As programs gain new features over time, continuously growing in complexity, program size can often become a critical issue for anything ranging from the context of tiny embedded devices up to extremely large programs in servers.
% As a result, reducing the code size is essential~\cite{schultz03,varma04,sehgal12,keoh14,auler17}.

%and compilation techniques must be developed and tuned primarily for optimising binary size.

Despite the importance of keeping code size small, compilers still make little effort to reduce it, except for some classical optimisations, such as dead-code elimination.
Their efforts are usually limited to disabling performance optimisations that tend to increase size, such as loop unrolling or inlining.
Developers might have more luck just removing functionality from their libraries~\cite{keoh14} or hand-optimizing their code~\cite{weaver09}.
However, these efforts are often undesirable if even possible at all.
Therefore, we must develop and tune compilation techniques primarily focused on reducing code size.

Code-size optimisations work by replacing a piece of code with another that is semantically equivalent but uses fewer or smaller instructions, in the binary format, sometimes combining and reusing equivalent pieces of code.
Classical optimisations that are effective in reducing code size include the elimination of redundant, unreachable, or dead code~\cite{cocke70,briggs97,debray00}.
Although initially motivated by performance, these classical optimisations achieve better performance by reducing the static number of instructions in the code, which translates to fewer dynamic instructions during runtime.

Recently, we have seen some progress with optimisations based on merging equivalent code within or across functions~\cite{edler14,chabbi21}.
One important optimisation capable of reducing code size is function merging.
In its simplest form, function merging reduces replicated code by combining multiple identical functions into a single one~\cite{llvm-fm,livska14}.
This optimisation is found in linkers, by the name of \textit{identical code folding}~(ICF), where text-identical functions at the bit level are merged~\cite{tallam10,kwan12,msvc-icf}.

It is obvious how function merging can reduce code size by removing function duplicates.
Nevertheless, it can also potentially reduce compilation time.
For example, when merging two identical functions, the remaining compilation pipeline will have one fewer function to process and optimise.
These benefits are not as obvious when merging non-identical functions as it introduces extra code, adding complexity to the merged function.
In the thesis, even though our main goal is reducing code size, we will focus on both dimensions: code size and compilation time.

\section{The Importance of Code Size for Different Domains}

In this section, we discuss in detail the importance of code size for different domains.

The embedded system market is rapidly growing.
Embedded systems need to perform increasingly complex tasks, with their application binaries often reaching several megabytes in size, while running on inexpensive and resource-constrained devices.
As a result, permanent storage and memory size becomes a limiting factor~\cite{plaza18}.
Just adding more memory is not always a viable option.
Highly integrated systems-on-chip are common in this market and their memories typically occupy the largest fraction of the chip area, contributing to most of the overall cost.
Even small increases in memory area translate directly to equivalent increases in cost, which lead to enormous levels of lost profit at large scales~\cite{edler10}.
In addition to cost, embedded systems are also often limited by other factors such as weight, area, and energy consumption~\cite{tiggeler00,edwards20}.
All these factors limit the size of the storage and memory available in embedded systems. 

%Modern mobile application binaries are bulky for many reasons: software and its dependencies, fast-paced addition of new features, high-level language constructs, and statically linked platform libraries.
Modern mobile applications tend to have large binaries that need to support as many devices as possible, including low-end devices with limited resources~\cite{hart02,etzo10}.
Furthermore, Apple App Store imposes a limit when downloading an application over the mobile broadband.
Applications larger than this limit must be downloaded only over the Wi-Fi.
These restrictions on the size of the application may significantly impact revenues for critical businesses.
Chabbi~et~al.~\cite{chabbi21} have shown that certain large applications may have over 90\%
of its total size being taken by their binary code, where the remaining size is due to media and resources.
For these applications, reducing the application's binary size becomes of utmost importance for their businesses.
%For these systems, reducing the binary size helps to improve the end-user experience but it is also critical for conforming with vendor's download-size limitations.
%Moreover, data consumption over wireless carriers can be either limited or expensive.
%As a result, there is an increasing focus on the development of programs tailored for these low-end devices with limited memory sizes~\cite{androidGo,hahm16}.

Beyond just mobile and embedded systems, powerful machines can also be unable to properly handle extremely large programs.
For example, compilation and load time can become impractical for extremely large programs and codebases~\cite{haas17,jaspan18}.
Moreover, address space also limits how large programs can become.
Upgrading computers at scale is a challenging and costly process even for large datacenters~\cite{yan16,neamtiu11}.
As a result, outdated 32-bit machines have an addressable memory space limited to less than 4~GB, setting a limit on program size.
This limitation is even worse on machines with shorter word widths.
In such constrained scenarios, reducing the application's footprint is essential~\cite{schultz03,varma04,sehgal12,keoh14,auler17}.

\section{Limitations of Existing Function Merging}

Google developed an optimization for the \textit{gold} linker that merges
functions that are identical at the bit-level~\cite{tallam10,kwan12}. 
Similar machine-level implementations are also offered by other production compilers
and linkers, such as MSVC~\cite{msvc-icf}.
% However, this machine-level solution is target-dependent and needs to be adapted for every back-end.
% A similar optimization for merging identical functions is offered at the IR level by both GCC and LLVM~\cite{llvm-fm,livska14}.
However, such solutions are platform-specific and need to be adapted for each object code format and hardware architecture.
Alternatively, compilers also provide a similar optimisation for merging identical functions at their mid-level intermediate representation (IR) which is therefore agnostic to the target hardware~\cite{llvm-fm,livska14}.
Unfortunately, these optimisations can only merge fully identical functions with at most type mismatches that can be losslessly cast to the same format.
These techniques can leverage their simplicity to efficiently identify groups of mergeable functions.
First they compute the hash of all functions, then a tree structure is used to group equivalent functions based on their hash values.

More advanced approaches can identify similar, but not necessarily identical, functions and replace them with a single function that combines the functionality of the original functions while eliminating redundant code.
At a high level, the way this works is that code specific to only one input function is added to the merged function but made conditional to a function identifier, while code found in both input functions is added only once and executed regardless of the function identifier.
%The work presented by von Koch~et~al.~\cite{edler14} proposed a merging strategy that exploits the isomorphism in the control-flow graphs (CFG) of the functions being merged.
%These functions can only differ between corresponding instructions, specifically, in their opcodes or the number and types of the input operands.
%However, they must have identical CFGs and function types.
The function-merging technique presented by von Koch~et~al.~\cite{edler14} exploits similarity among functions.
% Their optimization is able to merge similar functions that are not necessarily
% identical.
Two functions are structurally similar if both their function types are equivalent
and their control-flow graphs (CFGs) are isomorphic.
Two function types are equivalent if they agree in the number, order, and types
of their parameters as well as
their return types, linkage type, and other compiler-specific properties.
In addition to the structural similarity of the functions, their technique also
requires that corresponding basic blocks have exactly the same number of instructions
and that corresponding instructions must have equivalent resulting types.
% but may differ in their opcodes or in the number and type of their input operands.
Mergeable functions are only allowed to differ in corresponding instructions,
where they can differ in their opcodes or input operands.
%The only differences that are actually allowed is that
%corresponding instructions can 
%differ in their opcodes or in the number and type of their input operands.

%If two corresponding instructions have different opcodes, they split the basic
%block and insert a switch branch to select which instruction to execute
%depending on a function identifier.

% Because the state-of-the-art is limited to functions with identical CFGs
% and function types, once it merges a pair of functions, a third
% \textit{similar} function cannot be merged into the resulting merged function
% since they will differ in both CFGs and their lists of parameters.
% Due to this limiting factor, the state-of-the-art has to first collect all
% mergeable functions and merge them simultaneously.


%Although a simple and intuitive concept, it is crucial for making high-level abstractions usable, when they introduce duplicate code~\cite{tallam10,kwan12}.
%For example, some C++ ABIs may end up creating multiple identical constructors and destructors of a class to use in different contexts~\cite{kwan12} and C++ templates replicate code for different specialisations~\cite{tallam10,livska14}.
%More advanced approaches~\cite{edler14} have extended this idea into merging non-identical functions by leveraging structural similarity.
%Functions with identical control-flow graphs (CFGs) and only small differences within corresponding basic blocks are merged into a single function that maintains the semantics of the original functions.
%This is particularly important for handling specialised template functions with small differences in their compiled form.

%This includes optimisations that range from local to inter-procedural, such as:
%peephole optimisations that perform code simplification~\cite{tanenbaum82};
%elimination of unreachable or dead code~\cite{muchnick98};
%optimisations that reduce redundancies such as common-subexpression elimination and value numbering~\cite{cocke70,briggs97};
%procedural abstraction and function merging~\cite{loki04,edler10,rocha19}.

%Similar functions can arise for several reasons
%Generative programming~\cite{czarnecki99,draheim04}.
%Copy-and-paste programming~\cite{kim04,jablonski10,ahmed15}.

% Function merging reduces replicated code by combining multiple identical functions into a single one~\cite{llvm-fm,livska14}. 
% Although a simple and intuitive concept, it is crucial for making high-level
% abstractions usable, when they introduce duplicate code~\cite{tallam10,kwan12}.
% For example, some C++ ABIs may end up creating multiple identical constructors
% and destructors of a class to use in different contexts~\cite{kwan12} and C++
% templates replicate code for different specialisations~\cite{tallam10,livska14}.
% More advanced approaches~\cite{edler14} have extended this idea into
% merging non-identical functions by leveraging structural similarity. Functions
% with identical control-flow graphs (CFGs) and only small differences within
% corresponding basic blocks are merged into a single function that maintains
% the semantics of the original functions. This is particularly important for
% handling specialized template functions with small differences in their
% compiled form.

Unfortunately, existing approaches fail  to produce any noticeable code size reduction.
In this work, we introduce a novel way to merge functions that overcomes major limitations of existing techniques.
Our insight is that the weak results of existing function merging implementations are not due to the lack of duplicate code but due to the %rigid,
overly restrictive algorithms they use to find duplicates.

% While an improvement, even the state-of-the-art often usually fails to produce any
% noticeable code size reduction.
% In this paper, we introduce a novel way to merge
% functions that overcomes the major limitations of the state-of-the-art. Our
% insight is that the weak results of existing function merging implementations
% are not due to the lack of duplicate code but due to the rigid, overly restrictive
% algorithms they use to find duplicates.

Our approach is based upon the concept of sequence alignment, developed in bioinformatics for identifying functional or evolutionary relationships between different DNA or RNA sequences.
Similarly, we use sequence alignment to find areas of functional similarity in arbitrary function pairs.
Aligned segments with equivalent code are merged.
The remaining segments where the two functions differ are added to the new function too but have their code guarded by a function identifier.
This approach leads to significant code size reduction.
%more than three times better than the state-of-the-art can achieve.

Attempting to merge all pairs of functions is prohibitively expensive even for medium sized programs, considering the quadratic nature of sequence alignment.
To counter this, our technique is integrated with a ranking-based exploration mechanism that efficiently focuses the search to the most
promising pairs of functions. %\todo{whats interesting about this ranking?}.
As a result, we achieve our code size savings while introducing little compilation-time
overhead.

Compared to identical function merging, we introduce extra code to be executed,
namely the code that chooses between dissimilar sequences in merged functions.
A naive implementation could easily hurt performance, e.g by merging two hot functions
with only a few similarities.
We show that it is also possible to avoid performance degradation by incorporating
profiling information in the decision-making, enabling the compiler to avoid merging functions that contain hot code.
%, as shown in Chapter~\ref{chp:cgo19}.
%identify blocks of hot code and effectively minimise the overhead in this portion of the code.
%disable code size optimisations for them. 

\section{Contributions}

%This thesis consists of ideas and results which have been described in previous publications.
In this section, we provide an overview of the main contributions presented in this thesis.
We provide the first techniques capable of merging arbitrary pair of functions.
Then, we build on this technique to achieve an even greater code size reduction while also reducing its compilation-time overheads.
Our final contribution is a function merging technique that can effectively reduce code size as well as end-to-end compilation time.

\subsection{Function Merging by Sequence Alignment}

%The first technique capable of merging arbitrary pair of functions.

Existing function merging techniques fail to produce any noticeable code size reduction in most programs.
Our insight is that the weak results of existing function merging implementations are not due to the lack of duplicate code but due to the rigid, overly restrictive algorithms they use to find duplicates.
These techniques only work on either identical or mostly identical functions, limiting their potential to reduce code size.

As the first contribution of this thesis, we introduce a novel way to merge functions that lifts most of the restrictions imposed by prior techniques.
%We present a novel function merging optimisation for code size reduction.
Our technique is the first that allows merging arbitrary functions, including functions with different signatures and control flow graphs.
The proposed optimisation uses sequence alignment to identify code similarity and guide the merging operation.
It also merges parameters based on their usage, minimising the number of parameters and operand selection, and handles different return types using a union-like approach.
The goal is to maximise the amount of merged code while minimising the overhead required to handle the differences.

For functions with little to no similarity, merging them might increase code size.
However, because our technique is able to merge any pair of functions, it is necessary to identify which pairs of functions are the most profitable to merge.
To this end, we introduce a novel ranking mechanism for focusing our optimisation on function pairs that are more likely to be profitably merged.
The proposed mechanism first pre-computes fingerprints summarising each function and later use them to rank function candidates based on the fingerprint similarity.
For each function, merging will be only attempted for the top ranked candidates, significantly reducing compilation overhead while still resulting in a meaningful code size reduction.

These contributions have been previously described in the following publication:
\begin{itemize}
\item Rodrigo Rocha, Pavlos Petoumenos, Zheng Wang, Murray Cole, and Hugh Leather. ``Function merging by sequence alignment.'' In IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp. 149-163. 2019.
\end{itemize}

\subsection{Effective Function Merging in the SSA Form}

%Uncovers even more code size reduction.

Although our technique, introduced in Chapter~\ref{chp:cgo19}, achieves impressive results, it does not directly handle phi-nodes which are fundamental to the SSA form.
Instead, it applies register demotion to replace all such nodes with memory operations, in an attempt to simplify the code generation processes.
As we show in Chapter~\ref{chp:pldi20}, after register demotion, functions tend to be almost twice as long due to an excessive number of memory operations.
Therefore, such a strategy comes at the cost of poor merge results, larger memory footprint, and longer compilation time.

Chapter~\ref{chp:pldi20} presents our improved technique that avoids this pitfall with a new code generator capable of handling phi-nodes properly and completely bypassing register demotion.
%This leads to better code reduction performance and faster compilation time.
This approach results in better merged functions by not relying on later reversing the effects of register demotion.
Merging smaller functions also significantly reduces compilation overhead, due to the quadratic nature of sequence alignment.

The code generator also includes a novel phi-node coalescing optimisation tailored for function merging, reducing the total number of phi-nodes and easing the pressure on registers.
Phi-node coalescing is able to reduce the number of phi-nodes and selections, producing smaller merged functions and reducing code size even further.

These contributions have been previously described in the following publication:
\begin{itemize}
\item Rodrigo Rocha, Pavlos Petoumenos, Zheng Wang, Murray Cole, and Hugh Leather. ``Effective function merging in the SSA form.'' In ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pp. 854-868. 2020.
\end{itemize}

\subsection{Function Merging for Free}

%Function merging for free. lean on memory usage and reduces overall compilation time.

Our solution presented in Chapter~\ref{chp:pldi20}, SalSSA, achieves on average a 10\% code size reduction but at the cost of crippling compile-time inefficiencies, especially for large programs.
In Chapter~\ref{chp:lctes21}, we show that SalSSA can lead to 40\% slower compilation, taking up to 32~GB of peak memory usage even for modestly-sized program.
Such a resource requirement is beyond what is typically available to a developer and thus unsuitable for optimizing real-life programs.
These inefficiencies stem directly from its quadratic sequence alignment used to identify mergeable instructions in a pair of input functions.

In order to address these inefficiencies, we propose a new sequence alignment strategy that works on the basic block level.
Because basic blocks tend to  be much smaller than whole functions, this strategy greatly reduces the impact of the quadratic sequence alignment algorithm.
For even greater speedups, we propose a linear pairwise alignment that works on pairs of basic blocks of the same size. 
This technique is often enough to achieve good code size reduction, because profitably merged functions tend to have highly similar basic blocks.

We also propose a multi-tier profitability analysis.
This include a fine-grain analysis that estimates the profitability of the aligned basic blocks before actually generating their merged code,
allowing the compiler to bail out early from unprofitable merging attempts, speeding up the optimization process.
This fine-grain analysis on the aligned blocks result in improved code size reduction.

We show that this technique is capable of reducing end-to-end compilation time.
This result can be achieved due to two main reasons:
1) By merging functions and removing code duplicates, it reduces the overall amount of code that needs to be optimised and processed by the remaining compilation pipeline.
2) The compilation overhead required for merging functions is smaller than its benefits, resulting in a overall reduction on the end-to-end compilation time.

These contributions have been previously described in the following paper, which is currently under review:
\begin{itemize}
\item Rodrigo Rocha, Pavlos Petoumenos, Zheng Wang, Murray Cole, Hugh Leather, Kim Hazelwood. ``HyFM: Function Merging for Free.'' In the International Conference on Languages Compilers, Tools and Theory of Embedded Systems (LCTES). 2021.
\end{itemize}

\section{Structure}

This thesis is organised as follows:
\begin{description}

\item[Chapter~\ref{chp:background}] provides the main background. It provides an overview of compiler architecture and compiler optimisations for code size reduction.
It also provides terminology and describes the sequence alignment algorithm from bioinformatics.
Finally, it describes the deep learning techniques used in this work.

\item[Chapter~\ref{chp:relatedwork}] surveys the relevant literature. First we describe the classic compiler optimisations that work by reducing code size. Then, we describe in detail the existing techniques for function merging. Finally, we discuss other techniques related to identifying code similarity.
%Finally, we present key papers focused on applying machine learning for tuning heuristics that guide compiler optimisations.

\item[Chapter~\ref{chp:cgo19}] describes our novel function merging technique based on sequence alignment.
It also presents its accompanying search strategy, where a fingerprint-based ranking mechanism is used to focus the optimisation on functions with more similarities.

\item[Chapter~\ref{chp:pldi20}] describes our new code generator for function merging that is capable of effectively handling functions in the SSA form.
%, optimizing phi-nodes.

\item[Chapter~\ref{chp:lctes21}] describes our new alignment strategies that work on the basic block level, reducing compilation time overheads.
This enables a fine-grain multi-tier profitability analysis capable of bailing out early from unprofitable merging attempts.

%\item[Chapter~\ref{chp:deeplearning}] describes our two-tier profitability analysis based on deep learning and partial recompilation. Our partial recompilation mechanism is based on {\itercomp}, where we recompile sections of code to discover profitable merging opportunities. We also propose the use of deep learning to reduce the number of recompilations needed by predicting clearly unprofitable merging attempts.

\item[Chapter~\ref{chp:conclusion}] summarises the overall findings of the thesis and outlines potential avenues for future research.

\end{description}
