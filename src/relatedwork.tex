\chapter{Related Work} \label{chp:relatedwork}

\section{Code-Size Optimisations} \label{sec:relatedwork:codesize}

Although initially motivated by performance, many of the classical optimisations achieve better performance by reducing code size.
A small code, besides having fewer instructions to execute, can also have a positive impact on the cache utilisation.
Classical optimisations that are effective in reducing code size include the elimination of redundant, unreachable, and dead code, as well as certain kinds of strength reduction~\cite{cocke70,briggs97,debray00}.
In this section, we will describe some of these classical size-reducing optimisations.

\subsection{Constant Folding} \label{sec:relatedwork:constfold}

Constant folding is an optimisation that operates on the instruction level, identifying instructions whose operands are constant values, performing the evaluation of the instruction at compile time, and replacing it by the resulting value.
The effectiveness of constant folding can be augmented by combining it with constant propagation.
Constant folding reduces code size by eliminating instructions that can be computed at compile time.
Moreover, constant folding also works as an enabler to other optimisations, such as unreachable-code elimination (Section~\ref{sec:relatedwork:unreachable}).
 
\subsection{Unreachable-Code Elimination} \label{sec:relatedwork:unreachable}

Some functions may contain code that is unreachable. A code is unreachable if there is no valid control-flow path from the function's entry point that leads to it.
Since unreachable code is guaranteed to never be executed, compilers should remove it to avoid code bloat.

Often, unreachable code is uncovered by other optimisations.
For example, after constant propagation and constant folding, a conditional branch could have its condition evaluating to a constant, eliminating a path to one of its successor basic blocks.
If no other path leads to that basic block, it becomes unreachable.

The algorithm to eliminate unreachable code works in a mark-sweep manner, performing two passes over the basic blocks of the CFG.
The reachability analysis optimistically assumes that all basic blocks are dead until proven otherwise.
First, it marks all blocks as unreachable.
Next, starting from the entry point, it marks each block that it can reach as reachable.
If all branches and jumps are unambiguous, then all unmarked blocks can be deleted.
With ambiguous branches or jumps, the compiler must preserve any block that the branch or jump can reach.
This analysis is simple and inexpensive.

\subsection{Dead-Code Elimination} \label{sec:relatedwork:dce}

A value definition is dead if it is not used on any path from the point in which it is defined to the exit point of the function.
In a similar way, an instruction is dead if it computes only values that are not used on any execution path leading from the instruction.
Any dead definition or instruction can be simply removed without altering the program's semantics, therefore reducing code size~\cite{muchnick98}.

The algorithm to eliminate dead code has some similarities with that for unreachable-code elimination described in Section~\ref{sec:relatedwork:unreachable}.
This algorithm also works in a mark-sweep manner~\cite{cooper07}.
First, the algorithm marks \textit{critical} instructions as \textit{alive}.
An instruction is \textit{critical} if it has an observable effect, for example, if it is a return instruction, a branch instruction, a function call, a memory operation, any instruction with side effect, etc.
Then, the algorithm follows the \textit{use-def chain} of every alive instruction, marking the operand instructions as alive.
This process continues until no more instructions can be marked as alive.
Finally, the sweep phase removes all instructions that have not been marked as alive, reducing code size.

%\section{Code Compression}

%Previous work have proposed several techniques based on traditional data compression techniques~\cite{ernst97,cooper99compression,lekatsas98,lekatsas00,beszedes03}.

\section{Merging Identical Functions}

In this section, we will discuss existing optimisations for merging identical functions.
Figure~\ref{fig:example-identical} illustrates how identical functions can appear in real programs.
The first pair of functions, shown in Figure~\ref{fig:example-identical-1-sphinx3}, were extracted from the \texttt{482.sphinx3} benchmark.
The only difference between these two functions is in their parameter type.
However, all pointer types can be considered equivalent since they can be bitcast in a lossless way.
These functions are usually produced by copy-and-paste programming, where a given code pattern is copied and then repurposed~\cite{kim04,jablonski10,ahmed15}.
The second pair of functions, shown in Figure~\ref{fig:example-identical-2-gcc}, were extracted from the \texttt{403.gcc} benchmark and they are fully identical.
These functions are part of GCC's backend, where it is common to have code that is automatically generated from a machine description~\cite{muchnick98,kolek13,ghica15}.

\begin{figure}[h]
\centering
\begin{subfigure}{\textwidth}
\centering
\includegraphics[scale=0.9]{src/relatedwork/figs/example-identical-1-sphinx3}
\caption{Two semantically identical functions extracted from the \texttt{482.sphinx3} benchmark.}
\label{fig:example-identical-1-sphinx3}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\includegraphics[scale=0.9]{src/relatedwork/figs/example-identical-2-gcc}
\caption{Two semantically identical functions extracted from the \texttt{403.gcc} benchmark.}
\label{fig:example-identical-2-gcc}
\end{subfigure}
\caption{Example of identical functions.}
\label{fig:example-identical}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{src/relatedwork/figs/identical-example}
\caption{Two functions extracted from the \texttt{447.dealII} benchmark that are not identical at the source level, but after applying template specialisation and optimisations they become identical at the IR level.}
\label{fig:identical-example}
\end{figure}

Note, however, that functions can be identical at the IR or machine level without necessarily being identical at the source level.
Figure~\ref{fig:identical-example} shows two real functions extract from the
447.dealII program in the SPEC CPU2006~\cite{spec} benchmark suite.
Although these two functions are not identical at the source level, they become
identical after a template specialisation and some optimisations are applied, in
particular, constant propagation, constant folding, and dead-code elimination. 
Specialising \verb|dim| to $1$ enables to completely remove the loop in the
function \verb|PolynomialSpace|.
Similarly, specializing \verb|dim| to $1$ results in only the first iteration
of the loop in the function \verb|TensorProductPolynomials| being executed.
The compiler is able to statically analyze and simplify the loops in both
functions, resulting in the identical functions shown at the bottom of
Figure~\ref{fig:identical-example}.

Identical code is particularly common in C++ programs
with heavy use of \textit{parametric polymorphism}, via template or \textit{auto} type deduction.


\input{src/relatedwork/identical-code-folding}
\input{src/relatedwork/identical-merge}

\input{src/relatedwork/von-koch}

%\textbf{TODO: Comparison table: Identical vs VonKoch vs Ours}

\input{src/relatedwork/code-factoring}

\input{src/relatedwork/code-similarity}

%\input{src/relatedwork/feedback-directed}
%\input{src/relatedwork/deeptune}

